{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EditDistance.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Compute Levenshtein Distance between Text and Key Words for Spam Classification** "
      ],
      "metadata": {
        "id": "LjPh7W9aDL4q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDCyAkWDDJIa",
        "outputId": "60669c3f-13ce-4a47-db32-8a3f65b99ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149864 sha256=acaee54d9c405a796accd2f9dda103e0ee1ec4c525975a5ea085f5f941e24591\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.2\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Levenshtein import distance as lev\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# nltk.download('stemmer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUSa4ZcXDti9",
        "outputId": "57a25daf-1309-4032-ceb6-e30a4529ed3e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Error loading stemmer: Package 'stemmer' not found in\n",
            "[nltk_data]     index\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Keywords and Corpus"
      ],
      "metadata": {
        "id": "KamMwVcU0j3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Keywords for Levenshtein Distance\n",
        "keywords = {\"AST_Task2\":[\"Count\", \"Python\", \"given\", \"scikt\"], \"AST_Task3\":[], \"AST_Task4\":[]}"
      ],
      "metadata": {
        "id": "jqO4Een9D6Wh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the corpus to test\n",
        "corpus = 'CountVectorizer is a great tool provided by the scikit-learn library in Python. It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text. This is helpful when we have multiple such texts, and we wish to convert each word in each text into vectors (for using in further text analysis). Let us consider a few sample texts from a document (each as a list element):'"
      ],
      "metadata": {
        "id": "2QmPPP1clG3f"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing - Remove Punctuations and Stemming"
      ],
      "metadata": {
        "id": "QJk1eSKxD8MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import numpy as np\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Lowercase\n",
        "corpus = corpus.lower()\n",
        "\n",
        "# Punctuations\n",
        "corpus_alpha = \"\".join([char for char in corpus if char not in string.punctuation])\n",
        "\n",
        "# Tokenization\n",
        "words = word_tokenize(corpus_alpha)\n",
        "\n",
        "# Stopword Filtering\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filteredWords = [word for word in words if not word in stop_words]\n",
        "\n",
        "'''\n",
        "# Stemming\n",
        "# Optional - We can compare with and without Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmedWords = [stemmer.stem(word) for word in filteredWords]\n",
        "'''"
      ],
      "metadata": {
        "id": "zICm-NVxgBI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Levenshtein Matrix"
      ],
      "metadata": {
        "id": "evoFzoylztcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(filteredWords)\n",
        "\n",
        "levMatrix = np.zeros((len(filteredWords),len(keywords['AST_Task2'])))\n",
        "for text_word in range(len(filteredWords)):\n",
        "  for key_word in range(len(keywords['AST_Task2'])):\n",
        "    lev_dist = lev(filteredWords[text_word], keywords['AST_Task2'][key_word])\n",
        "    if filteredWords[text_word] == \"python\":\n",
        "      print(filteredWords[text_word])\n",
        "    if filteredWords[text_word] == \"given\":\n",
        "      print(keywords['AST_Task2'][key_word])\n",
        "    levMatrix[text_word][key_word] = lev_dist\n",
        "\n",
        "print(np.matrix(levMatrix))"
      ],
      "metadata": {
        "id": "JvZGVigFhtsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get count of occurence of each value in numpy array of non-negative ints\n",
        "# Get count of True elements in a numpy array\n",
        "for i in range(20):\n",
        "  bin_arr = np.bincount(levMatrix[i].dtype('int64'))\n",
        "  print(bin_arr[0]+bin_arr[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "x7CVJkab00v3",
        "outputId": "874637db-4b28-4eb7-ca9d-f5489398d770"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-2db75936d0ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Get count of True elements in a numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mbin_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevMatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbin_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.dtype[float64]' object is not callable"
          ]
        }
      ]
    }
  ]
}