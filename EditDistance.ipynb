{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EditDistance.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Compute Levenshtein Distance between Text and Key Words for Spam Classification** "
      ],
      "metadata": {
        "id": "LjPh7W9aDL4q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDCyAkWDDJIa",
        "outputId": "b1d0778b-cd3f-40ee-82f2-1cc26ce32fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (0.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Levenshtein import distance as lev\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUSa4ZcXDti9",
        "outputId": "32668dc1-a1d9-47f3-f01e-da140e45a00e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Keywords and Corpus"
      ],
      "metadata": {
        "id": "KamMwVcU0j3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Keywords for Levenshtein Distance\n",
        "keywords = {\"AST_Task2\":[\"Count\", \"Python\", \"given\", \"scikt\"], \"AST_Task3\":[], \"AST_Task4\":[]}"
      ],
      "metadata": {
        "id": "jqO4Een9D6Wh"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the corpus to test\n",
        "corpus = 'CountVectorizer is a great tool provided by the scikit-learn library in Python. It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text. This is helpful when we have multiple such texts, and we wish to convert each word in each text into vectors (for using in further text analysis). Let us consider a few sample texts from a document (each as a list element):'"
      ],
      "metadata": {
        "id": "2QmPPP1clG3f"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing - Remove Punctuations and Stemming"
      ],
      "metadata": {
        "id": "QJk1eSKxD8MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import numpy as np\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Lowercase\n",
        "corpus = corpus.lower()\n",
        "\n",
        "# Punctuations\n",
        "corpus_alpha = \"\".join([char for char in corpus if char not in string.punctuation])\n",
        "\n",
        "# Tokenization\n",
        "words = word_tokenize(corpus_alpha)\n",
        "\n",
        "# Stopword Filtering\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filteredWords = [word for word in words if not word in stop_words]\n",
        "\n",
        "# Stemming\n",
        "# Optional - We can compare with and without Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmedWords = [stemmer.stem(word) for word in filteredWords]"
      ],
      "metadata": {
        "id": "zICm-NVxgBI9"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the words\n",
        "print(\"FilteredWords\")\n",
        "print(filteredWords)\n",
        "print(\"StemmedWords\")\n",
        "print(stemmedWords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlezXgP350t4",
        "outputId": "dfa79ec6-3283-4ab2-a5b5-cea3efbd0f21"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FilteredWords\n",
            "['countvectorizer', 'great', 'tool', 'provided', 'scikitlearn', 'library', 'python', 'used', 'transform', 'given', 'text', 'vector', 'basis', 'frequency', 'count', 'word', 'occurs', 'entire', 'text', 'helpful', 'multiple', 'texts', 'wish', 'convert', 'word', 'text', 'vectors', 'using', 'text', 'analysis', 'let', 'us', 'consider', 'sample', 'texts', 'document', 'list', 'element']\n",
            "StemmedWords\n",
            "['countvector', 'great', 'tool', 'provid', 'scikitlearn', 'librari', 'python', 'use', 'transform', 'given', 'text', 'vector', 'basi', 'frequenc', 'count', 'word', 'occur', 'entir', 'text', 'help', 'multipl', 'text', 'wish', 'convert', 'word', 'text', 'vector', 'use', 'text', 'analysi', 'let', 'us', 'consid', 'sampl', 'text', 'document', 'list', 'element']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Levenshtein Matrix"
      ],
      "metadata": {
        "id": "evoFzoylztcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"FilteredWords\")\n",
        "count0=0\n",
        "count1=0\n",
        "count2=0\n",
        "count3=0\n",
        "levMatrix = np.zeros((len(filteredWords),len(keywords['AST_Task2'])))\n",
        "for text_word in range(len(filteredWords)):\n",
        "  for key_word in range(len(keywords['AST_Task2'])):\n",
        "    lev_dist = lev(filteredWords[text_word], keywords['AST_Task2'][key_word])\n",
        "    levMatrix[text_word][key_word] = lev_dist\n",
        "    if lev_dist == 0:\n",
        "      count0 += 1\n",
        "    if lev_dist == 1:\n",
        "      count1 += 1\n",
        "    if lev_dist == 2:\n",
        "      count2 += 1\n",
        "    if lev_dist == 3:\n",
        "      count3 += 1\n",
        "\n",
        "print(np.matrix(levMatrix))\n",
        "print(\".......................................................................\")\n",
        "\n",
        "print(\"StemmedWords\")\n",
        "counts0=0\n",
        "counts1=0\n",
        "counts2=0\n",
        "counts3=0\n",
        "levMatrix = np.zeros((len(stemmedWords),len(keywords['AST_Task2'])))\n",
        "for text_word in range(len(stemmedWords)):\n",
        "  for key_word in range(len(keywords['AST_Task2'])):\n",
        "    lev_dist = lev(stemmedWords[text_word], keywords['AST_Task2'][key_word])\n",
        "    levMatrix[text_word][key_word] = lev_dist\n",
        "    if lev_dist == 0:\n",
        "      counts0 += 1\n",
        "    if lev_dist == 1:\n",
        "      counts1 += 1\n",
        "    if lev_dist == 2:\n",
        "      counts2 += 1\n",
        "    if lev_dist == 3:\n",
        "      counts3 += 1\n",
        "\n",
        "print(np.matrix(levMatrix))\n",
        "print(\".......................................................................\")\n",
        "print(\"FilteredWords\")\n",
        "print(\"Edit Distance Count: 0: {}, 1: {}, 2: {}, 3: {}\".format(count0, count1, count2, count3))\n",
        "print(\"StemmedWords\")\n",
        "print(\"Edit Distance Count: 0: {}, 1: {}, 2: {}, 3: {}\".format(counts0, counts1, counts2, counts3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvZGVigFhtsV",
        "outputId": "aa257741-2f67-4044-e9ff-4b44cf1313a6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FilteredWords\n",
            "[[11. 13. 13. 13.]\n",
            " [ 4.  6.  4.  4.]\n",
            " [ 4.  4.  5.  5.]\n",
            " [ 7.  8.  6.  7.]\n",
            " [10.  9.  8.  6.]\n",
            " [ 7.  7.  6.  7.]\n",
            " [ 6.  1.  5.  6.]\n",
            " [ 5.  6.  4.  5.]\n",
            " [ 8.  8.  9.  8.]\n",
            " [ 5.  5.  0.  5.]\n",
            " [ 4.  5.  5.  4.]\n",
            " [ 6.  5.  6.  5.]\n",
            " [ 5.  6.  5.  5.]\n",
            " [ 7.  8.  7.  9.]\n",
            " [ 1.  6.  5.  4.]\n",
            " [ 4.  6.  5.  5.]\n",
            " [ 5.  6.  6.  5.]\n",
            " [ 6.  5.  5.  5.]\n",
            " [ 4.  5.  5.  4.]\n",
            " [ 7.  7.  7.  7.]\n",
            " [ 7.  7.  7.  7.]\n",
            " [ 5.  6.  5.  5.]\n",
            " [ 5.  5.  4.  4.]\n",
            " [ 5.  7.  5.  6.]\n",
            " [ 4.  6.  5.  5.]\n",
            " [ 4.  5.  5.  4.]\n",
            " [ 7.  6.  7.  6.]\n",
            " [ 4.  6.  5.  4.]\n",
            " [ 4.  5.  5.  4.]\n",
            " [ 8.  8.  8.  8.]\n",
            " [ 4.  5.  4.  4.]\n",
            " [ 4.  6.  5.  5.]\n",
            " [ 7.  8.  6.  7.]\n",
            " [ 6.  6.  6.  5.]\n",
            " [ 5.  6.  5.  5.]\n",
            " [ 4.  7.  6.  6.]\n",
            " [ 4.  6.  4.  3.]\n",
            " [ 5.  6.  5.  6.]]\n",
            ".......................................................................\n",
            "StemmedWords\n",
            "[[ 7.  9.  9. 10.]\n",
            " [ 4.  6.  4.  4.]\n",
            " [ 4.  4.  5.  5.]\n",
            " [ 5.  6.  5.  6.]\n",
            " [10.  9.  8.  6.]\n",
            " [ 7.  7.  6.  7.]\n",
            " [ 6.  1.  5.  6.]\n",
            " [ 4.  6.  4.  5.]\n",
            " [ 8.  8.  9.  8.]\n",
            " [ 5.  5.  0.  5.]\n",
            " [ 4.  5.  5.  4.]\n",
            " [ 6.  5.  6.  5.]\n",
            " [ 5.  6.  5.  5.]\n",
            " [ 6.  7.  6.  8.]\n",
            " [ 1.  6.  5.  4.]\n",
            " [ 4.  6.  5.  5.]\n",
            " [ 5.  6.  5.  4.]\n",
            " [ 5.  5.  5.  5.]\n",
            " [ 4.  5.  5.  4.]\n",
            " [ 5.  6.  5.  5.]\n",
            " [ 6.  6.  7.  6.]\n",
            " [ 4.  5.  5.  4.]\n",
            " [ 5.  5.  4.  4.]\n",
            " [ 5.  7.  5.  6.]\n",
            " [ 4.  6.  5.  5.]\n",
            " [ 4.  5.  5.  4.]\n",
            " [ 6.  5.  6.  5.]\n",
            " [ 4.  6.  4.  5.]\n",
            " [ 4.  5.  5.  4.]\n",
            " [ 7.  7.  7.  7.]\n",
            " [ 4.  5.  4.  4.]\n",
            " [ 4.  6.  5.  5.]\n",
            " [ 5.  6.  6.  6.]\n",
            " [ 5.  6.  5.  4.]\n",
            " [ 4.  5.  5.  4.]\n",
            " [ 4.  7.  6.  6.]\n",
            " [ 4.  6.  4.  3.]\n",
            " [ 5.  6.  5.  6.]]\n",
            ".......................................................................\n",
            "FilteredWords\n",
            "Edit Distance Count: 0: 1, 1: 2, 2: 0, 3: 1\n",
            "StemmedWords\n",
            "Edit Distance Count: 0: 1, 1: 2, 2: 0, 3: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N0goNmnW69OH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}